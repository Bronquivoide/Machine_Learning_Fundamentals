{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Regularización En Regresión Lineal con Scikit-learn**\n",
        "\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "La regularización es una técnica para evitar el *overfitting* en los modelos, en este caso de regresión lineal, y con ello mejorar su generalización.\n",
        "\n",
        "### **Regularización de Ridge (o regularización $L_2$)**\n",
        "\n",
        "Dada la función costo escrita anteriormente en el notebook de [regresión lineal múltiple](https://github.com/Bronquivoide/Machine_Learning_Fundamentals/blob/main/Supervised%20Learning/Regresi%C3%B3n%20Lineal/Regresi%C3%B3n_Lineal_M%C3%BAltiple.ipynb), $\\mathcal{L}(\\vec{w}) = \\frac{1}{N} \\lVert \\vec{f} - X \\vec{w} \\rVert ^2$, la regularización se logra añadiendo una penalización a los valores grandes de $\\vec{w}$.\n",
        "\n",
        "La función costo debe ser tal que los pesos asociados a $\\vec{w}$ sean acotados mediante una constante:\n",
        "\n",
        "$\\vec{w} ^T \\vec{w} \\geq C$\n",
        "\n",
        "Así, $\\mathcal{L}(\\vec{w})$ se puede reescribir de manera efectiva como:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(\\vec{w}) = \\frac{1}{N} \\lVert \\vec{f} - X \\vec{w} \\rVert ^2 + α \\lVert \\vec{w} \\rVert ^2\n",
        "$$\n",
        "\n",
        "En donde el término $α \\lVert \\vec{w} \\rVert ^2$ es el que penaliza los valores grandes de $\\vec{w}$.\n",
        "\n",
        "A esta forma analítica de regularización se le conoce como **Regularización de Ridge** o **Regularización $L_2$**.\n",
        "\n",
        "Con esta nueva $\\mathcal{L}(\\vec{w})$ se optimiza el vector de pesos, $\\vec{w} ^* = ArgMin_{\\vec{w}} \\{ \\mathcal{L}(\\vec{w}) \\}$, en donde tenemos que:\n",
        "\n",
        "$$\n",
        "\\vec{w}^* = (X^T X - α \\mathbb{I})^{-1}X^T \\vec{f}\n",
        "$$\n",
        "\n",
        "El término $X^T X - α \\mathbb{I}$ es una matriz regulada con la que $\\vec{w}$ resuelve el overfitting y la generalización del modelo.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **Otras regularizaciones**\n",
        "\n",
        "\n",
        "\n",
        "-  **Regularización de Lasso o $L_1$:**\n",
        "\n",
        "Aquí presenta la penalización en la función de costo mediante:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(\\vec{w}) = \\frac{1}{N} \\lVert \\vec{f} - X \\vec{w} \\rVert ^2 + α \\lVert \\vec{w} \\rVert\n",
        "$$\n",
        "\n",
        "-  **Regularización Elastic Net:**\n",
        "\n",
        "Esta regularización combina $L_1$ y $L_2$ como sigue:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(\\vec{w}) = \\frac{1}{N} \\lVert \\vec{f} - X \\vec{w} \\rVert ^2 + α _1 \\lVert \\vec{w} \\rVert + α_2 \\lVert \\vec{w} \\rVert ^2\n",
        "$$\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **Regresión regularizada en `scikit-learn`**\n",
        "\n",
        "$\\circ$ Sintaxis de regresión Ridge:\n",
        "\n",
        "\n",
        "```\n",
        "from sklearn.linear_model import Ridge\n",
        "ridge = Ridge(alpha)\n",
        "ridge.fit(X_train, y_train)\n",
        "y_pred = ridge.predict(X_test)\n",
        "```\n",
        "\n",
        "$\\circ$ Sintaxis de regresión Lasso:\n",
        "\n",
        "\n",
        "```\n",
        "from sklearn.linear_model import Lasso\n",
        "lasso = Lasso(alpha)\n",
        "lasso.fit(X_train, y_train)\n",
        "y_pred = lasso.predict(X_test)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "$\\circ$ Sintaxis de regresión Elastic Net:\n",
        "\n",
        "```\n",
        "from sklearn.linear_model import ElasticNet\n",
        "elastic = ElasticNet(alpha, l1_ratio)\n",
        "elastic.fit(X_train, y_train)\n",
        "y_pred = elastic.predict(X_test)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "Cuando se trata de un modelo de regresión regularizada, comunmente se [estandarizan](https://github.com/Bronquivoide/Machine_Learning_Fundamentals/blob/main/Preprocesamiento%20De%20Datos/Normalizaci%C3%B3n%20y%20Estandarizaci%C3%B3n/Normalizaci%C3%B3n_y_Estandarizaci%C3%B3n_de_Datos.ipynb) los datos que recibe.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **Hiperparámetro $α$:**\n",
        "\n",
        "El argumento `alpha` de los modelos anteriores es un valor numérico que corresponde al hiperparámetro $α$ que penaliza los pesos.\n",
        "\n",
        "Lo que queremos es hallar el modelo Ridge, Lasso o elastic Net con el $α$ asociado que nos regrese la mejor $R^2$.  Para esto normalmente el hiperparámetro se ajusta de dos maneras:\n",
        "\n",
        "**1) Se evaluan distintos valores de $α$**:\n",
        "\n",
        "Suele tomarse una lista de `alpha` en un ciclo `for` para evaluar el rendimiento del modelo.\n",
        "\n",
        "Por ejemplo, para una regularización Ridge:\n",
        "\n",
        "\n",
        "```\n",
        "from sklearn.linear_model import Ridge\n",
        "scores = []\n",
        "for alpha in [0.1, 1.0, 10.0, 100.0, 1000.0]:\n",
        "   ridge = Ridge(alpha=alpha)    \n",
        "   ridge.fit(X_train, y_train)\n",
        "   y_pred = ridge.predict(X_test)\n",
        "   scores.append(ridge.score(X_test, y_test))\n",
        "print(scores)\n",
        "\n",
        "```\n",
        "Esto nos regresa una lista con los valores de $R^2$ asociados a cada `alpha`.\n",
        "\n",
        "**2) Se ajusta $α$ mediante [$K-$Fold Cross-Validation ](https://github.com/Bronquivoide/Machine_Learning_Fundamentals/blob/main/Supervised%20Learning/Aprendizaje%20Supervisado%20con%20scikit-learn.pdf):**\n",
        "\n",
        "La validación cruzada por $K$-folds busca el mejor valor de `alpha` (entre un conjunto predefinido o uno especificado mediante una lista) y entrena un modelo final con ese `alpha` óptimo, usando todos los datos de entrenamiento.\n",
        "\n",
        "Por ejemplo, para una regularización Lasso:\n",
        "\n",
        "\n",
        "```\n",
        "from sklearn.linear_model import LassoCV\n",
        "lasso = LassoCV(cv) #Un valor típico es cv=10\n",
        "lasso.fit(X_train, y_train)\n",
        "print(\"El mejor alpha es:\", lasso.alpha_)\n",
        "```\n",
        "\n",
        "Así se regresa el modelo con el mejor rendimiento, es decir, con el mejor `alpha`. La sintaxis para Ridge es `import RidgeCV` y `import ElasticNetCV`.\n",
        "\n",
        "---\n",
        "\n",
        "### **Ejemplo: Regularización de Ridge para el dataset fetch_california_housing cardago en `scikit-learn`**\n",
        "\n",
        "`fetch_california_housing()` es un conjunto de datos datos recolectados mediante un censo en California en 1990. Es muy popular para prácticas de regresión, cuya finalidad es predecir el valor de una vivienda a partir de características demográficas y geográficas.\n",
        "\n"
      ],
      "metadata": {
        "id": "Mo8DxgvKMMTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "XQ0k2IsYMSXE"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cargando el dataset\n",
        "cal_house=fetch_california_housing()\n",
        "X, y=cal_house.data, cal_house.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#Estandarización de los valores\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "#Ridge con k-fold C.V.\n",
        "alphas=[0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
        "ridge_cv=RidgeCV(alphas=alphas, cv=10)\n",
        "ridge_cv.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(f'El mejor modelo es el que tiene alpha={ridge_cv.alpha_}, pues tiene un R2-score de {round(ridge_cv.score(X_test_scaled, y_test),2)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPLcyWKYlB-o",
        "outputId": "b156e8cb-95fd-4705-9213-bc9f352e4387"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El mejor modelo es el que tiene alpha=1.0, pues tiene un R2-score de 0.58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Los pesos asociados al modelo son: {ridge_cv.coef_}')\n",
        "print('°°°°°°°°°°°°°°°°°°°')\n",
        "\n",
        "print(f'Tenemos una matriz de diseño de orden {X.shape}, cuyas caracteríticas son: {cal_house.feature_names}')\n",
        "print('°°°°°°°°°°°°°°°°°°°')\n",
        "\n",
        "\n",
        "print(\"Valores reales:\", y_test)\n",
        "print('°°°°°°°°°°°°°°°°°°°')\n",
        "\n",
        "print(f\"Valores predichos por el modelo de Ridge:{ridge_cv.predict(X_test_scaled)}\" )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAX9otQVxIKz",
        "outputId": "778a777b-4669-426e-c2a6-ba9d0de49091"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Los pesos asociados al modelo son: [ 0.85432679  0.12262397 -0.29421036  0.33900794 -0.00228221 -0.04083302\n",
            " -0.89616759 -0.86907074]\n",
            "°°°°°°°°°°°°°°°°°°°\n",
            "Tenemos una matriz de diseño de orden (20640, 8), cuyas caracteríticas son: ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
            "°°°°°°°°°°°°°°°°°°°\n",
            "Valores reales: [0.477   0.458   5.00001 ... 5.00001 0.723   1.515  ]\n",
            "°°°°°°°°°°°°°°°°°°°\n",
            "Valores predichos por el modelo de Ridge:[0.71947224 1.76384666 2.709309   ... 4.46847645 1.18797174 2.00922052]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ohPZbviZxW6c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}